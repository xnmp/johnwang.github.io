---
layout: post
title: Support Vector Machines
---

About SVMs:

<script type="text/javascript" async
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script>


## Linear SVM

The basic motivating example for a linear support vector machine goes
something like: suppose we have a dataset with two features, each
datapoint is of either class $A$ or $B$, and we need to predict
the class of each datapoint. 

Geometrically this corresponds to having a set of points on the plane,
each being either red or blue, and we need to predict the color of
a point based on its location. That is we have a set of points 

{% raw %}


$$\mathbb{x}_{i}$$

where $i$ indexes the datapoints. For each $i$ we also have a target
$y_{i}$ which is either -1 or 1. We can think of -1 as being red
and 1 as being blue, but the reason we choose these two numbers (as
opposed to say, 0 and 1) will become clear later. 

The goal of a linear SVM is to find a line such that points on the
left of the line are red, and points on the right are blue, and such
that the distance between this line and the closest point of each
class is minimized. 

{% endraw %}
